{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zSFOng81x3Y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DistilBERT Classification - Suite du preprocessing\n",
        "# Ce notebook continue aprÃ¨s le preprocessing des donnÃ©es\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 1 : Imports et configuration\n",
        "# ============================================================================\n",
        "\n",
        "# Installation des packages nÃ©cessaires (dÃ©commenter si besoin)\n",
        "# !pip install transformers torch datasets scikit-learn\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    DistilBertTokenizer,\n",
        "    DistilBertForSequenceClassification,\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Configuration du device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Utilisation du device: {device}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 2 : PrÃ©paration des donnÃ©es depuis df (continuitÃ© du preprocessing)\n",
        "# ============================================================================\n",
        "\n",
        "# On rÃ©cupÃ¨re les colonnes 'text' et 'label_encoded' du df crÃ©Ã© prÃ©cÃ©demment\n",
        "# Assurez-vous d'avoir exÃ©cutÃ© le notebook de preprocessing avant !\n",
        "\n",
        "print(f\"\\nNombre total d'Ã©chantillons: {len(df)}\")\n",
        "print(f\"\\nDistribution des classes:\")\n",
        "print(df['label_encoded'].value_counts())\n",
        "\n",
        "# Mapping des labels\n",
        "label_names = {0: 'Politics', 1: 'Sport', 2: 'Tech'}\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 3 : Division Train/Validation/Test\n",
        "# ============================================================================\n",
        "\n",
        "# Division train/temp (80/20)\n",
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
        "    df['text'].values,\n",
        "    df['label_encoded'].values,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['label_encoded'].values\n",
        ")\n",
        "\n",
        "# Division temp en val/test (50/50 de temp = 10/10 du total)\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    temp_texts,\n",
        "    temp_labels,\n",
        "    test_size=0.5,\n",
        "    random_state=42,\n",
        "    stratify=temp_labels\n",
        ")\n",
        "\n",
        "print(f\"\\nðŸ“Š Division des donnÃ©es:\")\n",
        "print(f\"  Train: {len(train_texts)} Ã©chantillons ({len(train_texts)/len(df)*100:.1f}%)\")\n",
        "print(f\"  Validation: {len(val_texts)} Ã©chantillons ({len(val_texts)/len(df)*100:.1f}%)\")\n",
        "print(f\"  Test: {len(test_texts)} Ã©chantillons ({len(test_texts)/len(df)*100:.1f}%)\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 4 : Classe Dataset PyTorch personnalisÃ©e\n",
        "# ============================================================================\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    \"\"\"Dataset PyTorch pour les articles de presse\"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenization avec DistilBERT\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "print(\"\\nâœ… Classe Dataset crÃ©Ã©e\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 5 : Initialisation du tokenizer et crÃ©ation des datasets\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nðŸ”„ Chargement du tokenizer DistilBERT...\")\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# CrÃ©ation des datasets\n",
        "print(\"ðŸ“¦ CrÃ©ation des datasets...\")\n",
        "train_dataset = NewsDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = NewsDataset(val_texts, val_labels, tokenizer)\n",
        "test_dataset = NewsDataset(test_texts, test_labels, tokenizer)\n",
        "\n",
        "print(\"âœ… Datasets crÃ©Ã©s avec succÃ¨s!\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 6 : Configuration des hyperparamÃ¨tres\n",
        "# ============================================================================\n",
        "\n",
        "# HyperparamÃ¨tres\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_LABELS = 3\n",
        "MAX_LENGTH = 512\n",
        "\n",
        "print(f\"\\nâš™ï¸ HyperparamÃ¨tres:\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Epochs: {EPOCHS}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Max length: {MAX_LENGTH}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 7 : CrÃ©ation des DataLoaders\n",
        "# ============================================================================\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "print(f\"\\nðŸ“š DataLoaders crÃ©Ã©s:\")\n",
        "print(f\"  Batches d'entraÃ®nement: {len(train_loader)}\")\n",
        "print(f\"  Batches de validation: {len(val_loader)}\")\n",
        "print(f\"  Batches de test: {len(test_loader)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 8 : Initialisation du modÃ¨le DistilBERT\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nðŸ¤– Chargement du modÃ¨le DistilBERT...\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    'distilbert-base-uncased',\n",
        "    num_labels=NUM_LABELS\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# Optimiseur AdamW\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Learning rate scheduler\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "print(f\"âœ… ModÃ¨le chargÃ© sur {device}\")\n",
        "print(f\"ðŸ“ˆ Total steps: {total_steps}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 9 : Fonctions d'entraÃ®nement et d'Ã©valuation\n",
        "# ============================================================================\n",
        "\n",
        "def train_epoch(model, data_loader, optimizer, scheduler, device):\n",
        "    \"\"\"EntraÃ®nement pour une Ã©poque\"\"\"\n",
        "    model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    progress_bar = tqdm(data_loader, desc='Training')\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Calcul de l'accuracy\n",
        "        _, preds = torch.max(logits, dim=1)\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Mise Ã  jour de la progress bar\n",
        "        progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    accuracy = correct_predictions.double() / len(data_loader.dataset)\n",
        "    avg_loss = np.mean(losses)\n",
        "\n",
        "    return accuracy, avg_loss\n",
        "\n",
        "\n",
        "def eval_model(model, data_loader, device):\n",
        "    \"\"\"Ã‰valuation du modÃ¨le\"\"\"\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc='Evaluating'):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            _, preds = torch.max(logits, dim=1)\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    accuracy = correct_predictions.double() / len(data_loader.dataset)\n",
        "    avg_loss = np.mean(losses)\n",
        "\n",
        "    return accuracy, avg_loss\n",
        "\n",
        "print(\"\\nâœ… Fonctions d'entraÃ®nement dÃ©finies\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 10 : Boucle d'entraÃ®nement\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nðŸš€ DÃ©but de l'entraÃ®nement...\\n\")\n",
        "\n",
        "history = {\n",
        "    'train_acc': [],\n",
        "    'train_loss': [],\n",
        "    'val_acc': [],\n",
        "    'val_loss': []\n",
        "}\n",
        "\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'{\"=\"*60}')\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "    print(f'{\"=\"*60}')\n",
        "\n",
        "    # EntraÃ®nement\n",
        "    train_acc, train_loss = train_epoch(\n",
        "        model,\n",
        "        train_loader,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    print(f'\\nðŸ“Š Train - Loss: {train_loss:.4f} | Accuracy: {train_acc:.4f}')\n",
        "\n",
        "    # Validation\n",
        "    val_acc, val_loss = eval_model(model, val_loader, device)\n",
        "\n",
        "    print(f'ðŸ“Š Val   - Loss: {val_loss:.4f} | Accuracy: {val_acc:.4f}\\n')\n",
        "\n",
        "    # Sauvegarde de l'historique\n",
        "    history['train_acc'].append(train_acc.cpu().numpy())\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_acc'].append(val_acc.cpu().numpy())\n",
        "    history['val_loss'].append(val_loss)\n",
        "\n",
        "    # Sauvegarde du meilleur modÃ¨le\n",
        "    if val_acc > best_accuracy:\n",
        "        torch.save(model.state_dict(), 'best_distilbert_model.bin')\n",
        "        best_accuracy = val_acc\n",
        "        print(f'ðŸ’¾ Meilleur modÃ¨le sauvegardÃ©! Accuracy: {best_accuracy:.4f}')\n",
        "\n",
        "print(\"\\nâœ… EntraÃ®nement terminÃ©!\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 11 : Visualisation des courbes d'entraÃ®nement\n",
        "# ============================================================================\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Graphique Accuracy\n",
        "ax1.plot(range(1, EPOCHS+1), history['train_acc'], 'b-o', label='Train', linewidth=2)\n",
        "ax1.plot(range(1, EPOCHS+1), history['val_acc'], 'r-o', label='Validation', linewidth=2)\n",
        "ax1.set_title('Accuracy par Ã©poque', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('Ã‰poque', fontsize=12)\n",
        "ax1.set_ylabel('Accuracy', fontsize=12)\n",
        "ax1.legend(fontsize=11)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_xticks(range(1, EPOCHS+1))\n",
        "\n",
        "# Graphique Loss\n",
        "ax2.plot(range(1, EPOCHS+1), history['train_loss'], 'b-o', label='Train', linewidth=2)\n",
        "ax2.plot(range(1, EPOCHS+1), history['val_loss'], 'r-o', label='Validation', linewidth=2)\n",
        "ax2.set_title('Loss par Ã©poque', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('Ã‰poque', fontsize=12)\n",
        "ax2.set_ylabel('Loss', fontsize=12)\n",
        "ax2.legend(fontsize=11)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_xticks(range(1, EPOCHS+1))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š Graphiques sauvegardÃ©s dans 'training_history.png'\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 12 : Ã‰valuation sur le test set\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Ã‰VALUATION SUR LE TEST SET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Charger le meilleur modÃ¨le\n",
        "model.load_state_dict(torch.load('best_distilbert_model.bin'))\n",
        "print(\"\\nâœ… Meilleur modÃ¨le chargÃ©\")\n",
        "\n",
        "def get_predictions(model, data_loader, device):\n",
        "    \"\"\"Obtenir les prÃ©dictions du modÃ¨le\"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    real_values = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc='PrÃ©diction'):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "\n",
        "            _, preds = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            real_values.extend(labels.cpu().numpy())\n",
        "\n",
        "    return predictions, real_values\n",
        "\n",
        "# Obtenir les prÃ©dictions\n",
        "y_pred, y_test = get_predictions(model, test_loader, device)\n",
        "\n",
        "# Calcul des mÃ©triques\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nðŸŽ¯ Accuracy sur le test set: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "\n",
        "print(\"\\nðŸ“‹ Rapport de classification dÃ©taillÃ©:\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(\n",
        "    y_test,\n",
        "    y_pred,\n",
        "    target_names=['Politics', 'Sport', 'Tech'],\n",
        "    digits=4\n",
        "))\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 13 : Matrice de confusion\n",
        "# ============================================================================\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Blues',\n",
        "    xticklabels=['Politics', 'Sport', 'Tech'],\n",
        "    yticklabels=['Politics', 'Sport', 'Tech'],\n",
        "    cbar_kws={'label': 'Nombre de prÃ©dictions'}\n",
        ")\n",
        "plt.title('Matrice de Confusion - DistilBERT', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.ylabel('Vraie classe', fontsize=12)\n",
        "plt.xlabel('Classe prÃ©dite', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š Matrice de confusion sauvegardÃ©e dans 'confusion_matrix.png'\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 14 : Analyse par classe\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ANALYSE DÃ‰TAILLÃ‰E PAR CLASSE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, label_name in enumerate(['Politics', 'Sport', 'Tech']):\n",
        "    mask = np.array(y_test) == i\n",
        "    class_accuracy = accuracy_score(\n",
        "        np.array(y_test)[mask],\n",
        "        np.array(y_pred)[mask]\n",
        "    )\n",
        "    total_samples = mask.sum()\n",
        "    correct_samples = (np.array(y_test)[mask] == np.array(y_pred)[mask]).sum()\n",
        "\n",
        "    print(f\"\\nðŸ“Œ {label_name}:\")\n",
        "    print(f\"   Ã‰chantillons: {total_samples}\")\n",
        "    print(f\"   Correct: {correct_samples}\")\n",
        "    print(f\"   Accuracy: {class_accuracy:.4f} ({class_accuracy*100:.2f}%)\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 15 : Fonction de prÃ©diction pour nouveaux textes\n",
        "# ============================================================================\n",
        "\n",
        "def predict_news(text, model, tokenizer, device, max_length=512):\n",
        "    \"\"\"\n",
        "    PrÃ©dire la catÃ©gorie d'un article de presse\n",
        "\n",
        "    Args:\n",
        "        text (str): Texte de l'article\n",
        "        model: ModÃ¨le DistilBERT entraÃ®nÃ©\n",
        "        tokenizer: Tokenizer DistilBERT\n",
        "        device: Device (CPU/GPU)\n",
        "        max_length (int): Longueur maximale des tokens\n",
        "\n",
        "    Returns:\n",
        "        tuple: (classe prÃ©dite, probabilitÃ©s)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenization\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    # PrÃ©diction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "        _, prediction = torch.max(logits, dim=1)\n",
        "\n",
        "    predicted_class = label_names[prediction.item()]\n",
        "    probabilities = {\n",
        "        label_names[i]: probs[0][i].item()\n",
        "        for i in range(NUM_LABELS)\n",
        "    }\n",
        "\n",
        "    return predicted_class, probabilities\n",
        "\n",
        "print(\"\\nâœ… Fonction de prÃ©diction dÃ©finie\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 16 : Test avec des exemples\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TEST AVEC DES EXEMPLES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_examples = [\n",
        "    \"The football team won the championship after a great match against their rivals.\",\n",
        "    \"New artificial intelligence breakthrough could revolutionize computer vision technology.\",\n",
        "    \"The government announced new economic policies to tackle inflation and unemployment.\"\n",
        "]\n",
        "\n",
        "for i, text in enumerate(test_examples, 1):\n",
        "    print(f\"\\nðŸ“„ Exemple {i}:\")\n",
        "    print(f\"Texte: {text[:100]}...\")\n",
        "\n",
        "    predicted_class, probabilities = predict_news(text, model, tokenizer, device)\n",
        "\n",
        "    print(f\"\\nðŸŽ¯ PrÃ©diction: {predicted_class}\")\n",
        "    print(f\"\\nðŸ“Š ProbabilitÃ©s:\")\n",
        "    for label, prob in sorted(probabilities.items(), key=lambda x: x[1], reverse=True):\n",
        "        print(f\"   {label}: {prob:.4f} ({prob*100:.2f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… NOTEBOOK TERMINÃ‰ AVEC SUCCÃˆS!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "F-07ZnoFY1Nm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}